{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lgIXSZJ1w9gi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b5d253f4-3108-46d0-dcee-b9a85745dfee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mxnet\n","  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.9/dist-packages (from mxnet) (1.22.4)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.9/dist-packages (from mxnet) (2.27.1)\n","Collecting graphviz<0.9.0,>=0.8.1\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.12.7)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.20.1\n","    Uninstalling graphviz-0.20.1:\n","      Successfully uninstalled graphviz-0.20.1\n","Successfully installed graphviz-0.8.4 mxnet-1.9.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gluonnlp\n","  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.5/344.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.5.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from gluonnlp) (1.22.4)\n","Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from gluonnlp) (0.29.34)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from gluonnlp) (23.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp39-cp39-linux_x86_64.whl size=680541 sha256=58dbd5821ad6ea076133baf6dbcd739d21e8b4a00c18783d7569c5c10930576b\n","  Stored in directory: /root/.cache/pip/wheels/47/17/70/b257bc53879a458c4bfcc900e89271aa8b4f19366a54bd2455\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.10.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.98\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==3.0.2\n","  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.0/769.0 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.9/dist-packages (from transformers==3.0.2) (0.1.98)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from transformers==3.0.2) (1.22.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from transformers==3.0.2) (23.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==3.0.2) (2.27.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==3.0.2) (4.65.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==3.0.2) (2022.10.31)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==3.0.2) (3.11.0)\n","Collecting tokenizers==0.8.1.rc1\n","  Downloading tokenizers-0.8.1rc1.tar.gz (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/97.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==3.0.2) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==3.0.2) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==3.0.2) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==3.0.2) (2.0.12)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==3.0.2) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==3.0.2) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==3.0.2) (1.2.0)\n","Building wheels for collected packages: tokenizers, sacremoses\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n","\u001b[0m  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=e395f9953874887b019dacfea7f9e36d203ae5750ffa55d075fa906308bea1c8\n","  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n","Successfully built sacremoses\n","Failed to build tokenizers\n","\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.11.0)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n","  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-g_5772px\n","  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-g_5772px\n","  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting boto3<=1.15.18\n","  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from kobert==0.2.3) (0.10.0)\n","Collecting mxnet<=1.7.0.post2,>=1.4.0\n","  Downloading mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnxruntime<=1.8.0,==1.8.0\n","  Downloading onnxruntime-1.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece<=0.1.96,>=0.1.6\n","  Downloading sentencepiece-0.1.96-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch<=1.10.1,>=1.7.0\n","  Downloading torch-1.10.1-cp39-cp39-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers<=4.8.1,>=4.8.1\n","  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.9/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (23.3.3)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.22.4)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.20.3)\n","Collecting s3transfer<0.4.0,>=0.3.0\n","  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting botocore<1.19.0,>=1.18.18\n","  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.34)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (23.1)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.9/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.27.1)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (0.8.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.11.0)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.65.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2022.10.31)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n","Collecting sacremoses\n","  Using cached sacremoses-0.0.53-py3-none-any.whl\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n","Collecting urllib3<1.26,>=1.20\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (3.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.2.0)\n","Building wheels for collected packages: kobert\n","  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15705 sha256=964416474fec5dad7494e4819da72a6679cf5ca5d47b970da82b311b063b7428\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ixrkrsr8/wheels/0b/20/d8/031374f3d29b5150c59c814bed091fca7d6d4c8218148bf286\n","Successfully built kobert\n","Installing collected packages: tokenizers, sentencepiece, urllib3, torch, sacremoses, onnxruntime, jmespath, botocore, s3transfer, mxnet, huggingface-hub, transformers, boto3, kobert\n","  Attempting uninstall: sentencepiece\n","    Found existing installation: sentencepiece 0.1.98\n","    Uninstalling sentencepiece-0.1.98:\n","      Successfully uninstalled sentencepiece-0.1.98\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.26.15\n","    Uninstalling urllib3-1.26.15:\n","      Successfully uninstalled urllib3-1.26.15\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.0.0+cu118\n","    Uninstalling torch-2.0.0+cu118:\n"]}],"source":["# Install Package\n","\n","!pip install mxnet\n","!pip install gluonnlp pandas tqdm\n","!pip install sentencepiece\n","!pip install transformers==3.0.2\n","!pip install torch\n","!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8B-2xew2E6es"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMAX0prI9DfF"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import progressbar\n","import time\n","# Import Library\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","from tqdm import tqdm, tqdm_notebook\n","from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O9joPTGbxHZR"},"outputs":[],"source":["# Set GPU environment\n","device = torch.device(\"cuda:0\")\n","# Install KoBert\n","bertmodel, vocab = get_pytorch_kobert_model()\n","\n","# Set BERT Tokenizer\n","\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))\n","\n","## Setting parameters\n","max_len = 64\n","batch_size = 64\n","warmup_ratio = 0.1\n","num_epochs = 5\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate =  5e-5\n","\n","#토큰화\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","\n","# KoBERT Modeling.\n","\n","class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes=5, ##Categories. 0 = Very Negative, 1 = Negative, 2 = Neutral, 3 = Positive, 4 = Very Positive.\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)\n","\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jyFBR7RGxq9r"},"outputs":[],"source":["PATH = '/content/drive/MyDrive/Project/LiveCommerse/OpenAI/KoBERT/'\n","model = torch.load(PATH + 'KoBERT_GPT-turn.pt')  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n","model.load_state_dict(torch.load(PATH + 'model_state_dict.pt')) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"weVWsps6-V80"},"outputs":[],"source":["main_path = '/content/drive/MyDrive/Project/LiveCommerse/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Irjalq7Hyioz"},"outputs":[],"source":["Path_Beauty = '/content/drive/MyDrive/Project/LiveCommerse/FUCKUGOOGLE/Beauty/Video'\n","Path_Food = '/content/drive/MyDrive/Project/LiveCommerse/FUCKUGOOGLE/Food/Video'\n","Path_Life = '/content/drive/MyDrive/Project/LiveCommerse/FUCKUGOOGLE/Life/Video'\n","Path_Fashion = '/content/drive/MyDrive/Project/LiveCommerse/FUCKUGOOGLE/Fashion/Video'\n","Beauty_list = os.listdir(Path_Beauty)\n","Food_list = os.listdir(Path_Food)\n","Life_list = os.listdir(Path_Life)\n","Fashion_list = os.listdir(Path_Fashion)\n","path_list = [Path_Beauty, Path_Food, Path_Life, Path_Fashion]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fVa-P7_y0-wO"},"outputs":[],"source":["# 학습된 KoBERT 불러온 뒤 예측.\n","\n","def predict(predict_sentence):\n","\n","    data = [predict_sentence, '0']\n","    dataset_another = [data]\n","    \n","    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n","    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n","    \n","    model.eval()\n","\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","\n","        out = model(token_ids, valid_length, segment_ids)\n","        prediction = out.cpu().detach().numpy().argmax()\n","\n","    return prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZsSuTkKNWj6N"},"outputs":[],"source":["map_dict = dict({4: 2, 3: 1, 2: 0, 1: -1, 0: -2})\n","for path in path_list[2:3]:\n","  total_df = pd.DataFrame()\n","  video_list = os.listdir(path)\n","  for a, video in enumerate(video_list):\n","    video_num = video.split('_')[1]\n","    video_num = video_num.split('.')[0]\n","    video_path = (path + '/' + video)\n","    try:\n","      video_df = pd.read_csv(video_path)\n","      bar = progressbar.ProgressBar(maxval = len(video_df)).start()\n","      for i in range(len(video_df)):\n","        bar.update(i)\n","        video_df.loc[i, 'KoBERT_Sentiment_2'] = predict(video_df.loc[i, 'chat'])\n","        video_df['KoBERT_Sentiment_1'] = video_df['KoBERT_Sentiment_2'].map(lambda x: map_dict.get(x))\n","        video_df.to_csv(video_path)\n","      print(f'Done, {video_path}')\n","      bar.finish()\n","    except:\n","      print(f'wrong, {video_path}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ol093DzwEOEg"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}